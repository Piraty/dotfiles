#!/bin/sh
# find_duplicate_files [PATH..] - report duplicate files

#DESIGN
#  1. find files of equal size
#  2. for files with equal size, compare hashsum
#  files with the same hash are printed on consequtive lines, separated by blank line

: "${HASH:=md5sum}"

for arg; do
	case "$arg" in
		--md5) HASH=md5sum ;;
		--sha1) HASH=sha1sum ;;
		--sha256) HASH=sha256sum ;;
		--sha512) HASH=sha512sum ;;
		*) break ;;
	esac
	shift
done

[ "$#" -gt 0 ] || set -- "."

# list files with their filesize
filelist_size="$(mktemp)"
for arg; do
	find "$arg" -type f -print0 | du -b --files0-from=-
done >"$filelist_size"

# cleanup list: remove empty files
sed -i "$filelist_size" -e '/^0/d'

# list files of same size and same hashsum
duplicate_sizes="$(cut -f1 "$filelist_size" | sort -rn | uniq -d)"
[ -n "$duplicate_sizes" ] || exit
for size in $duplicate_sizes; do
	# save hashsum for each file of equal size
	filelist_hash="${filelist_size}_${size}"
	grep "^${size}[ 	]" "$filelist_size" | cut -f2- | parallel -j "$(nproc)" "$HASH" >"$filelist_hash"

	# print duplicate files
	sep=":"
	duplicate_hashes="$(cut -d' ' -f1 "$filelist_hash" | sort | uniq -d)"
	[ -n "$duplicate_hashes" ] || continue
	for hash in $duplicate_hashes; do
		# printf '%s:' "$size"
		grep "^$hash" "$filelist_hash" | cut -d' ' -f3- | sort | tr '\n' "$sep" | sed -e "s/${sep}$//"
		printf '\n'
	done
done

# cleanup
rm "$filelist_size"*
