#!/bin/sh
# find_duplicate_files PATH.. - report duplicate files

#DESIGN
#  1. find files of equal size
#  2. compare their hashsum
#  equal file are printed on the same line, separated by a whitespace

: "${HASH:=md5sum}"

for arg; do
	case "$arg" in
		--md5) HASH=md5sum ;;
		--sha1) HASH=sha1sum ;;
		--sha256) HASH=sha256sum ;;
		--sha512) HASH=sha512sum ;;
		*) break ;;
	esac
	shift
done

# generate list of files with their filesize
filelist_size="$(mktemp)"
for arg; do
	find "$arg" -type f | parallel -j "$(nproc)" du
done >"$filelist_size"

# generate list of files of same size and their hashsum
duplicate_sizes="$(cut -f1 "$filelist_size" | sort -n | uniq -d)"
echo "$duplicate_sizes" | parallel grep
for size in $duplicate_sizes; do
	filelist_hash="${filelist_size}_${size}"
	grep "^$size[ 	]" "$filelist_size" | cut -f2 | parallel -j "$(nproc)" "$HASH" >"$filelist_hash"
	duplicate_hash="$(cut -d' ' -f1 "$filelist_hash" | sort -n | uniq -d)"

	# print duplicate files
	if [ -n "$duplicate_hash" ]; then
		for hash in $duplicate_hash; do
			grep "^$hash" "$filelist_hash" |
				cut -d' ' -f3- |
				sed 's/ /\\ /g' |
				tr '\n' ' '
		done
		printf '\n'
	fi
done

# cleanup
rm "$filelist_size"*
